{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(size_x, sizes_h, size_y):\n",
    "    \"\"\" Initializes weights and bias units for nn.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size_x: int\n",
    "        Size of input layer, X\n",
    "    sizes_h: int or list \n",
    "        Size(s) of hidden layer. If more than a single hidden layer,\n",
    "        a list of layer sizes must be supplied.\n",
    "    size_y: int\n",
    "        Size of output layer, Y\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        dictionary of parameters with keys W1, W2,... Wn and b1, b2,... bn\n",
    "    \"\"\"\n",
    "    if type(sizes_h) == list:\n",
    "        layer_sizes = [size_x] + sizes_h + [size_y]\n",
    "    elif type(sizes_h) == int:\n",
    "        layer_sizes = [size_x] + [sizes_h] + [size_y]\n",
    "    else:\n",
    "        raise TypeError('Invalid parameter: sizes_h must be an array or an int')\n",
    "\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        parameters['W' + str(i)] = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * 0.01\n",
    "        parameters['b' + str(i)] = np.zeros(layer_sizes[i])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_relu(Z):\n",
    "    \"\"\" ReLu activation function.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    Z: numpy array\n",
    "        Can be any shape.\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    numpy matrix:\n",
    "        Post-activation parameter of same shape as Z.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_sigmoid(Z):\n",
    "    \"\"\" Sigmoid activation function.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    Z: numpy array\n",
    "        Can be any shape.\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "        Post-activation parameter of same shape as Z.\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(dJ_dA, Z):\n",
    "    \"\"\" The gradient of a ReLu unit during backpropogation.\n",
    "    \n",
    "    Formula for gradient: dJ/dZ = dJ/dA * dA/dZ\n",
    "    dA/dZ is also known as g'(Z)\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    dJ_dA: numpy array\n",
    "        The post-activation gradient (any shape).\n",
    "    \n",
    "    Z: numpy array\n",
    "    \n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    numpy array:\n",
    "        The derivitive dJ_dZ (i.e. gradient of the cost with respect to Z, dJ/dZ) of same shape as dJ_dA.\n",
    "    \"\"\"\n",
    "    # note: g(Z) = derivative of ReLu = `np.where(Z > 0, 1, 0)`\n",
    "    dJ_dZ = dJ_dA * np.where(Z > 0, 1, 0)\n",
    "    \n",
    "    assert dJ_dZ.shape == dJ_dA.shape\n",
    "    return dJ_dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(dJ_dA, Z):\n",
    "    \"\"\" The gradient of a sigmoid unit during backpropogation.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    dJ_dA: numpy array\n",
    "        The post-activation gradient (any shape).\n",
    "        \n",
    "    Z: numpy array\n",
    "    \n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    numpy array:\n",
    "        The derivitive dJ_dZ (i.e. gradient of the cost with respect to Z, dJ/dZ) of same shape as dJ_dA.\n",
    "    \n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-Z))\n",
    "    # note: g(Z) = derivative of sigmoid = `(sigmoid * (1 - sigmoid))`\n",
    "    dJ_dZ = dJ_dA * (sigmoid * (1 - sigmoid))\n",
    "    \n",
    "    assert dJ_dZ.shape == dJ_dA.shape\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
